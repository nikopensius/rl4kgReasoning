# -*- coding: utf-8 -*-
"""proofOfConcept.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DfHYWlfaMzG15Bd-fIE1r20M0iIZ_-vU
"""

!pip install ampligraph
!pip uninstall tensorflow
!pip install tensorflow==1.15

import os
os.chdir('/content/drive/My Drive/Colab Notebooks/thesis/deeppath/FB15k-237/tasks/Combined')

import pandas as pd
import pickle
filepath = "action_space.pkl"
action_space = pickle.load(open(filepath, "rb"))
print("Length of action_space pre FOBO removal:", len(action_space))
test = pd.read_csv("test.csv").drop("Unnamed: 0", axis=1)
FOBO = list(test.rr.unique())
FOBO_inv = [relation + "_inv" for relation in FOBO]
FOBO.extend(FOBO_inv)

action_space = [action for action in action_space if action not in FOBO]
print("Length of action_space post FOBO removal:", len(action_space))
print("Saving updated action_space into", filepath)
pickle.dump(action_space, open(filepath, "wb"))

import pickle
import pandas as pd
import numpy as np
import ampligraph
from ampligraph.latent_features import restore_model
import copy
"""
from google.colab import drive
drive.mount('/content/drive')
"""
import os
os.chdir("/content/drive/MyDrive/Colab Notebooks/thesis/deeppath/NELL-995/tasks/")
"""
os.chdir('/content/drive/My Drive/Colab Notebooks/thesis/deeppath/FB15k-237/tasks')

### Read triples and pairs ###
load = 1

go_to = 36 #0-34
dir = os.listdir()
os.chdir('./' + dir[go_to])
!pwd
"""
dir = os.listdir()
for i, f in enumerate(dir):
  print(i, f)
go_to = int(input())
os.chdir('./' + dir[go_to])

load = 1
if not load:
  kg_triples = read_kg_triples()  # used by agent to look for steps in path
  pickle.dump(kg_triples, open("kg_triples.pkl", "wb"))
  input = read_input()    # split into train and test set
  input.to_csv("input.csv")
  df_kg = read_triples()  # used for training KG
  df_kg.to_csv("df_kg")
  train_pos = train_pos()
  pickle.dump(train_pos, open("train_pos.pkl", "wb"))

kg_triples = pickle.load(open("kg_triples.pkl", "rb"))
inputt = pd.read_csv("input.csv")

for index, data in inputt.iterrows():
  pair = data.es + data.rr
  if pair in kg_triples:
    del kg_triples[pair]

to_del = []
for key in kg_triples:
  if ("type_of_union" in key) or ("category" in key):
    to_del.append(key)

for key in to_del:
  del kg_triples[key]

pickle.dump(kg_triples, open("kg_triples.pkl", "wb"))

"""##REINFORCEMENT LEARNING"""

import torch  
import gym
import numpy as np  
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt
import random

# Constants
GAMMA = 0.95 ###################################################################################

class PolicyNetwork(nn.Module):
    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=1e-5, manual_seed = 0):
        if manual_seed:      # set seed manually for controlled experimentla environment
          torch.manual_seed(manual_seed)
          np.random.seed(manual_seed)
        super(PolicyNetwork, self).__init__()
        self.train = True
        self.num_actions = num_actions
        self.linear1 = nn.Linear(num_inputs, hidden_size) #####################################
        self.linear2 = nn.Linear(hidden_size, num_actions) ####################################
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate) ######################
        self.illegal_ent = "/m/08mbj5d"

    def forward(self, state):
        x = F.relu(self.linear1(state.flatten()))
        x = F.softmax(self.linear2(x))
        return x
    
    def get_val_action(self, probs, action_space, current_ent, kg):
        while True:
          # get index of most probable action = maxProb_i,
          maxProb_i = probs.argmax()

          # retrieve most probable action
          action = action_space[maxProb_i]

          # check if most probable action leads to a new entity
          key = current_ent + action

          if key in kg and kg[key] != [self.illegal_ent]:
              return maxProb_i
          else:
            # disqualify the maximum probability from further choice
            probs[maxProb_i] = -1
    

    #"""
    # get_val_actions - a variation for BEAM search
    def get_val_actions(self, probs, action_space, current_ent, kg, b):
        max_is = []
        for _ in range(len(probs)):
          # get index of most probable action = maxProb_i,
          maxProb_i = probs.argmax()

          # retrieve most probable action
          action = action_space[maxProb_i]

          # check if most probable action leads to a new entity
          key = current_ent + action
          if key in kg and kg[key] != [self.illegal_ent]:
              max_is.append(maxProb_i)
          # disqualify the maximum probability from further choice
          probs[maxProb_i] = -1
          
          if len(max_is) == b:
            break
        #if not max_is:
          #print(current_ent)
        return max_is

    def get_actions(self, state, action_space, current_ent, kg_triples, b=1):
        # Convert state to tensor
        state = torch.from_numpy(state).float().unsqueeze(0)
        # Calculate probabilities over action space
        probs = self.forward(Variable(state))
        probs_array = probs.detach().numpy()
        copy_of_probs = copy.deepcopy(probs_array)  #to be used in highest_prob_action
                                                    #the problem is, the thing will not know that its top probabilities lead to nowhere                                              

        highest_prob_actions = self.get_val_actions(copy_of_probs, action_space, current_ent, kg_triples, b)
      
        jou = probs.squeeze(0).squeeze(0)
        log_probs = []
        for hpa in highest_prob_actions:
          joujou = jou[hpa]
          log_prob = torch.log(joujou)
          log_probs.append(log_prob)
        return highest_prob_actions, log_probs
            
    #"""


    def get_random_val_action(self, probs, action_space, current_ent, kg):
        no_good = []
        while True:
          # get index of random action
          randProb_i = self.random_arg(action_space, probs)
          if randProb_i in no_good:
            continue

          # retrieve the action
          action = action_space[randProb_i]

          # check if the action leads to a new entity
          key = current_ent + action
          if key in kg and kg[key] != [self.illegal_ent]:
            return randProb_i
          else:
            no_good.append(randProb_i)

    def random_arg(self, action_space, action_probs):
      return np.random.choice(np.arange(len(action_space)), p = np.squeeze(action_probs))

    def get_random_val_action(self, probs, action_space, current_ent, kg):
        while True:
          # get index of random action
          randProb_i = self.random_arg(action_space, probs)

          # retrieve the action
          action = action_space[randProb_i]

          # check if the action leads to a new entity
          key = current_ent + action
          if kg[key] != [self.illegal_ent]:
            return randProb_i
    
    def mask_probs(self, action_space, current_ent, kg_triples, probs):
        for i, action in enumerate(action_space):
          key = current_ent + action
          if key not in kg_triples:
            probs[i] = 0
        return probs / sum(probs)
    
    def normal_distribution(self, highest_prob_actions, probs):
        highest_probs = []
        for hpi in highest_prob_actions:
          highest_probs.append(probs[hpi])
        highest_probs = np.asarray(highest_probs)
        highest_probs = highest_probs / sum(highest_probs)
        return highest_probs

    def get_action(self, state, action_space, current_ent, kg_triples):
        # Convert state to tensor
        state = torch.from_numpy(state).float().unsqueeze(0)
        # Calculate probabilities over action space
        probs = self.forward(Variable(state))
        probs_array = probs.detach().numpy()
        copy_of_probs = copy.deepcopy(probs_array)  #to be used in highest_prob_action
                                                    #the problem is, the thing will not know that its top probabilities lead to nowhere
        
        rand = False
        if rand:
          masked_copy_of_probs = self.mask_probs(action_space, current_ent, kg_triples, copy_of_probs)
          highest_prob_action = self.get_random_val_action(masked_copy_of_probs, action_space, current_ent, kg_triples)#.item()
        else:
          #highest_prob_action = self.get_val_action(copy_of_probs, action_space, current_ent, kg_triples)
          highest_prob_actions = self.get_val_actions(copy_of_probs, action_space, current_ent, kg_triples, 1) #NELL-995 change
          if not highest_prob_actions:
            print(current_ent)
          distribution = self.normal_distribution(highest_prob_actions, copy.deepcopy(probs_array))
          highest_prob_action = highest_prob_actions[np.random.choice(np.arange(len(highest_prob_actions)), p = distribution)]
      
        jou = probs.squeeze(0).squeeze(0)
        joujou = jou[highest_prob_action]
        log_prob = torch.log(joujou)
        return highest_prob_action, log_prob




def update_policy(policy_network, rewards, log_probs):
    discounted_rewards = []

    for t in range(len(rewards)):
        Gt = 0 
        pw = 0
        for r in rewards[t:]:
            Gt = Gt + GAMMA**pw * r
            pw = pw + 1
        discounted_rewards.append(Gt)
        
    discounted_rewards = torch.tensor(discounted_rewards)
    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards
                                                                                                        ##################################

    policy_gradient = []
    for log_prob, Gt in zip(log_probs, discounted_rewards):
        policy_gradient.append(-log_prob * Gt)
    torch.autograd.set_detect_anomaly(True)
    policy_network.optimizer.zero_grad()
    policy_gradient = torch.stack(policy_gradient).sum()
    policy_gradient.backward()
    policy_network.optimizer.step()





from numpy import dot
from numpy.linalg import norm


# Use this
import pickle
import random
from ampligraph.latent_features import restore_model

class Environment_train():
  
  def __init__(self, path_to_dataset, path_to_model):
    self.model_name   = path_to_model
    self.i            = 2
    self.states       = None
    self.train_pos    = pickle.load(open("train_pos.pkl", "rb"))
    print(os.listdir())
    self.action_space = pickle.load(open("action_space.pkl", "rb"))
    self.action_space.append("selfLoop")
    self.input        = pd.read_csv(path_to_dataset)
    self.model        = restore_model(path_to_model)          # This is a KG model
    self.kg           = pickle.load(open("kg_triples.pkl", "rb"))
    self.stats        = {"Set size:": self.input.shape[0],
                         "Size of action space:" : 0,
                         "Number of episodes:": 0,                          # env.stats["Number of episodes:"]
                         "Target found" : 0}
    n_input, n_hidden, n_out, batch_size, learning_rate = 440, 300, 1, 1, 0.01

    self.classification_model = nn.Sequential(nn.Linear(n_input, n_hidden),
                      nn.ReLU(),
                      nn.Linear(n_hidden, n_out),
                      nn.Sigmoid())
    self.loss_function = nn.MSELoss()
    self.optimizer = torch.optim.SGD(self.classification_model.parameters(), lr=learning_rate)

  def __get_state(self, i):
    if type(i) != int:
      i = random.randint(0, self.input.shape[0] - 1)
    row = self.input.iloc[i]
    triple = [row.et, row.rr, row.es]
    #FB15K-237
    #self.actual_target = self.train_pos[row.es]
    #NELL-995
    self.actual_target = self.train_pos[row.es+row.rr]
    self.source = row.es
    self.relation = row.rr
    self.claimed_target = row.et
    hr_state = triple + [0, 0, 0, 0, 0, 0, 0, 0]
    y = row.y

    source_and_target_embeddings = self.model.get_embeddings([triple[0], triple[2]])
    relation_embedding = self.model.get_embeddings([triple[1]], "relation")[0]
    state = np.zeros((11, 40))
    state[0] = source_and_target_embeddings[0]
    state[1] = relation_embedding
    state[2] = source_and_target_embeddings[1]
    
    return [hr_state], [state], y

  def reset(self, i = None):
    self.hr_states, self.states, self.y = self.__get_state(i)      # this is a list
    self.i = 2
    self.complex_source = self.complex_vector(self.source)
    self.complex_relation = self.complex_vector(self.relation)
    return self.hr_states, self.states

  def __get_embedding(self, entity_ID, emb_type="entity"):
    #print(entity_ID)
    return self.model.get_embeddings([entity_ID], emb_type)


  def complex_vector(self, string):
    object_type = "relation"
    #if string[0] == "/":
    try:
      test_coords = string.split(",")
      for el in test_coords:
        float(el)
      object_type = "entity"
    except:
      if ("_" in string):
        object_type = "entity"
    try: embedding = self.model.get_embeddings([string], object_type)[0]
    except:
      print(string, object_type)
    return self.complexify(embedding)

  def dot_product(self, candidate):

    source = self.complex_source
    relation = self.complex_relation
    candidate = self.complex_vector(candidate).conjugate()

    dot_product = 0
    for u, a, e in zip(source, relation, candidate):
      dot_product += u * a * e
    return dot_product.real

  def complexify(self, input_vector):
    output_vector = []
    for real, imag in zip(input_vector[:20], input_vector[20:]):
      comp = complex(real, imag)
      output_vector.append(comp)
    output_vector = np.asarray(output_vector)
    return output_vector

  def __cosine_similarity(self, candidate):

    source = self.model.get_embeddings([self.source])[0]
    relation = self.model.get_embeddings([self.relation], "relation")[0]
    target = source + relation
    
    return dot(candidate, target) / (norm(candidate)*norm(target))

# 21.08.2022
# Experiment no 2:
#  ComplEx embedding with entity choice cos(claimed target, candidate)

  def __cosine_similarity(self, candidate):

    target = self.model.get_embeddings([self.claimed_target])[0]
    
    return dot(candidate, target) / (norm(candidate)*norm(target))
  
  def query(self, entity, action):
    key = entity + action
    return self.kg[key]
  
  def extend_state(self, hr_state, state, action):
    state = copy.deepcopy(state)
    hr_state = copy.deepcopy(hr_state)
    current_ent = hr_state[self.i]
    new_ent_candidates = self.query(current_ent, action)
    new_ent, new_ent_embed = self.best_candidate(new_ent_candidates)

    hr_state[self.i + 1] = action
    hr_state[self.i + 2] = new_ent
    state[self.i + 1] = self.__get_embedding(action, "relation")[0]
    state[self.i + 2] = new_ent_embed

    return hr_state, state, new_ent

    
  def sigmoid(self, x):
    return 1 / (1 + np.exp(-x))

  def best_candidate(self, candidates):
    if "/m/08mbj5d" in candidates:
      candidates.remove("/m/08mbj5d")
    candidates_embedded = self.model.get_embeddings(candidates)
    if self.model_name == "TransE":
      similarities = map(self.__cosine_similarity, candidates_embedded)
      similarities = np.asarray(list(similarities))
      index_of_most_similar = similarities.argmax()
    elif self.model_name == "ComplEx":
      random = False
      if random:
        dot_products = map(self.dot_product, candidates)
        dot_products = np.asarray(list(dot_products))
        sigmoid_same = map(self.sigmoid, dot_products)
        sigmoid_same = np.asarray(list(sigmoid_same))
        scores = []
        indices = []
        for _ in range(3):
          index_of_most_similar = sigmoid_same.argmax()
          indices.append(sigmoid_same.argmax())
          scores.append(sigmoid_same.max())
          sigmoid_same[index_of_most_similar] = 0
        scores = np.asarray(scores)
        scores /= sum(scores)
        index_of_most_similar = np.random.choice(indices, p=scores)
        new_ent = candidates[index_of_most_similar]
        new_ent_embedded = candidates_embedded[index_of_most_similar]
      else:
        dot_products = map(self.dot_product, candidates)
        dot_products = np.asarray(list(dot_products))
        index_of_most_similar = dot_products.argmax()
    new_ent = candidates[index_of_most_similar]
    new_ent_embedded = candidates_embedded[index_of_most_similar]
    return new_ent, new_ent_embedded

  """
# 21.08.2022
# Experiment no 1:
#  ComplEx embedding with entity choice cos(TransE scoring function based virtual target, candidate)
#  yields a high precision because ComplEx is a good embedding
#  and the TransE

  def best_candidate(self, candidates):
    if "/m/08mbj5d" in candidates:
      candidates.remove(policy_net.illegal_ent)
    candidates_embedded = self.model.get_embeddings(candidates)
    similarities = map(self.__cosine_similarity, candidates_embedded)
    similarities = np.asarray(list(similarities))
    index_of_most_similar = similarities.argmax()
    new_ent = candidates[index_of_most_similar]
    new_ent_embedded = candidates_embedded[index_of_most_similar]
    return new_ent, new_ent_embedded
  """
  def step(self, hr_state, state, action_index):   #BRING IN THE STATE

    reward = 0
    
    action = self.action_space[action_index]
    new_hr_state, new_state, new_ent = self.extend_state(hr_state, state, action)

    if new_ent in self.actual_target: # BLIND CHANGE
      reward = 1

    self.states[0][self.i + 1] = self.model.get_embeddings([action], "relation")
    self.hr_states[0][self.i + 1] = action

    self.states[0][self.i + 2] = self.model.get_embeddings([new_ent])[0]
    self.hr_states[0][self.i + 2] = new_ent
    
    return new_hr_state, new_state, reward #for new_state, return and embedding of the entity (which is the state)






import numpy as np
from numpy import dot
from numpy.linalg import norm

# Use this
import pickle
import random
from ampligraph.latent_features import restore_model

class Environment_test():
  
  def __init__(self, path_to_dataset, path_to_model):
    self.model_name   = path_to_model
    self.i            = 2
    self.states       = None
    self.train_pos    = pickle.load(open("train_pos.pkl", "rb"))
    self.action_space = pickle.load(open("action_space.pkl", "rb"))
    self.action_space.append("selfLoop")
    self.input        = pd.read_csv(path_to_dataset).sample(frac=0.3, random_state=1)
    self.model        = restore_model(path_to_model)          # This is a KG model
    self.kg           = pickle.load(open("kg_triples.pkl", "rb"))
    self.stats        = {"Set size:": self.input.shape[0],
                         "Size of action space:" : 0,
                         "Number of episodes:": 0,                          # env.stats["Number of episodes:"]
                         "Target found" : 0}
    n_input, n_hidden, n_out, batch_size, learning_rate = 440, 300, 1, 1, 0.01

    self.classification_model = nn.Sequential(nn.Linear(n_input, n_hidden),
                      nn.ReLU(),
                      nn.Linear(n_hidden, n_out),
                      nn.Sigmoid())
    self.loss_function = nn.MSELoss()
    self.optimizer = torch.optim.SGD(self.classification_model.parameters(), lr=learning_rate)

  def __get_state(self, i):
    if type(i) != int:
      i = random.randint(0, self.input.shape[0] - 1)
    row = self.input.iloc[i]
    triple = [row.et, row.rr, row.es]
    # FB15K-237
    #self.actual_target = self.train_pos[row.es]
    # NELL-995
    self.actual_target = self.train_pos[row.es + row.rr]
    self.source = row.es
    self.relation = row.rr
    self.claimed_target = row.et
    hr_state = triple + [0, 0, 0, 0, 0, 0, 0, 0]
    y = row.y

    source_and_target_embeddings = self.model.get_embeddings([triple[0], triple[2]])
    relation_embedding = self.model.get_embeddings([triple[1]], "relation")[0]
    state = np.zeros((11, 40))
    state[0] = source_and_target_embeddings[0]
    state[1] = relation_embedding
    state[2] = source_and_target_embeddings[1]
    
    return [hr_state], [state], y

  def reset(self, i = None):
    self.hr_states, self.states, self.y = self.__get_state(i)      # this is a list
    self.i = 2
    self.complex_source = self.complex_vector(self.source)
    self.complex_relation = self.complex_vector(self.relation)
    return self.hr_states, self.states

  def __get_embedding(self, entity_ID, emb_type="entity"):
    return self.model.get_embeddings([entity_ID], emb_type)

  def complex_vector(self, string):
    object_type = "relation"
    #if string[0] == "/":
    try:
      test_coords = string.split(",")
      for el in test_coords:
        float(el)
      object_type = "entity"
    except:
      if ("_" in string):
        object_type = "entity"
    embedding = self.model.get_embeddings([string], object_type)[0]
    return self.complexify(embedding)

  def dot_product(self, candidate):

    source = self.complex_source
    relation = self.complex_relation
    candidate = self.complex_vector(candidate).conjugate()

    dot_product = 0
    for u, a, e in zip(source, relation, candidate):
      dot_product += u * a * e
    return dot_product.real

  def complexify(self, input_vector):
    output_vector = []
    for real, imag in zip(input_vector[:20], input_vector[20:]):
      comp = complex(real, imag)
      output_vector.append(comp)
    output_vector = np.asarray(output_vector)
    return output_vector

  def __cosine_similarity(self, candidate):

    source = self.model.get_embeddings([self.source])[0]
    relation = self.model.get_embeddings([self.relation], "relation")[0]
    target = source + relation
    
    return dot(candidate, target) / (norm(candidate)*norm(target))

# 21.08.2022
# Experiment no 2:
#  ComplEx embedding with entity choice cos(claimed target, candidate)

  def __cosine_similarity(self, candidate):

    target = self.model.get_embeddings([self.claimed_target])[0]
    
    return dot(candidate, target) / (norm(candidate)*norm(target))
  
  def query(self, entity, action):
    key = entity + action
    return self.kg[key]
  
  def extend_state(self, hr_state, state, action, b):
    state = copy.deepcopy(state)
    hr_state = copy.deepcopy(hr_state)
    current_ent = hr_state[self.i]
    new_ent_candidates = self.query(current_ent, action)
    new_ents, new_ents_embed, scores = self.best_candidate(new_ent_candidates, b)

    hr_state[self.i + 1] = action
    state[self.i + 1] = self.__get_embedding(action, "relation")[0]

    hr_states = []
    states = []
    for i in range(len(new_ents)):
      h = copy.deepcopy(hr_state)
      h[self.i + 2] = new_ents[i]
      #h = np.asarray(h)
      hr_states.append(h)
      s = copy.deepcopy(state)
      #s = np.asarray(s)
      s[self.i + 2] = new_ents_embed[i]
      states.append(s)

    return hr_states, states, new_ents, scores

  def sigmoid(self, x):
    return 1 / (1 + np.exp(-x))

  def best_candidate(self, candidates, b):
    if "/m/08mbj5d" in candidates:
      candidates.remove("/m/08mbj5d")
    candidates_embedded = self.model.get_embeddings(candidates)
    if self.model_name == "TransE":
      similarities = map(self.__cosine_similarity, candidates_embedded)
      similarities = np.asarray(list(similarities))
      index_of_most_similar = similarities.argmax()
    elif self.model_name == "ComplEx":
      dot_products = map(self.dot_product, candidates)
      dot_products = np.asarray(list(dot_products))
      sigmoid_same = map(self.sigmoid, dot_products)
      sigmoid_same = np.asarray(list(sigmoid_same))
      new_ents = []
      new_ents_embedded = []
      scores = []
      for _ in range(b):
        index_of_most_similar = sigmoid_same.argmax()
        scores.append(sigmoid_same.max())
        sigmoid_same[index_of_most_similar] = 0
        new_ent = candidates[index_of_most_similar]
        new_ent_embedded = candidates_embedded[index_of_most_similar]
        new_ents.append(new_ent)
        new_ents_embedded.append(new_ent_embedded)
    return new_ents, new_ents_embedded, scores
  """
# 21.08.2022
# Experiment no 1:
#  ComplEx embedding with entity choice cos(TransE scoring function based virtual target, candidate)
#  yields a high precision because ComplEx is a good embedding
#  and the TransE

  def best_candidate(self, candidates):
    if "/m/08mbj5d" in candidates:
      candidates.remove(policy_net.illegal_ent)
    candidates_embedded = self.model.get_embeddings(candidates)
    similarities = map(self.__cosine_similarity, candidates_embedded)
    similarities = np.asarray(list(similarities))
    index_of_most_similar = similarities.argmax()
    new_ent = candidates[index_of_most_similar]
    new_ent_embedded = candidates_embedded[index_of_most_similar]
    return new_ent, new_ent_embedded
  """
  def step(self, hr_state, state, action_index):   #BRING IN THE STATE

    reward = 0
    
    action = self.action_space[action_index]
    new_hr_states, new_states, new_ents, scores = self.extend_state(hr_state, state, action, b)

    for new_ent in new_ents:
      if new_ent in self.actual_target: # BLIND CHANGE
        reward = 1
    
    return new_hr_states, new_states, reward, new_ents, scores #for new_state, return and embedding of the entity (which is the state)





import random

def train_model(path_to_model, max_episode_num, max_steps, random_state = 0, manual_seed = 0, policy_net = None, start = 0):
    # Maybe these two should be chosen from a random title?
    # Or maybe another agent should choose them?
    # How should it learn to choose?
    #   Maybe using embeddings to decide(learn)?

    size_observation_space = 440 #(len(list(relevante_aliases.keys())))
    env = Environment_train("train.csv", path_to_model)
    size_action_space = len(env.action_space)
    if not policy_net:
      policy_net = PolicyNetwork(size_observation_space, size_action_space, 128, manual_seed = manual_seed)
    
    len_dataset = env.input.shape[0]
    env.stats["Number of episodes:"] = max_episode_num
    env.stats["Size of action space:"] = size_action_space
    max_steps = max_steps
    all_rewards = []
    avg_rewards = []

    # collect confidence scores to calculate confidence thresholds for true and fake classifications
    confidence_scores = {"true_closed"  : [], # y = +, hr[0] == hr[-1]
                         "fake_closed"  : [], # y = -, hr[0] == hr[-1] max_score([fake_closed]) < closed_path.score --> closed_path.verity = true
                         "true_open"    : [], # y = +, hr[0] != hr[-1] max_score([true_open]) < open_path.score --> open_path.verity = fake
                         "fake_open"    : []} # y = -, hr[0] != hr[-1]

    # FOR DIVERSITY REWARD CALCULATION
    #past_paths = []

    for episode in range(start + 1, max_episode_num):
        # at the beggining of each go-through, shuffle the train dataset
        i = episode % len_dataset
        x = episode // len_dataset
        if i == 0:
          if random_state:
            env.input = env.input.sample(frac = 1, random_state = random_state)
          else:
            env.input = env.input.sample(frac = 1)

        hr_states, states = env.reset(i)

        log_probs = []
        rewards = []

        for steps in range(max_steps): # [0, 1, 2, 3]
          new_hr_states     = []
          new_states        = []

          for hr_state, state in zip(env.hr_states, env.states):
            current_ent       = hr_state[env.i]
            action, log_prob  = policy_net.get_action(state, env.action_space, current_ent, env.kg)

            new_hr_state, new_state, reward = env.step(hr_state, state, action)

            # collect the updated states
            new_hr_states.append(new_hr_state)
            new_states.append(new_state)

            # for the Policy Network
            # collect log_prob and reward for 1 step
            log_probs.append(log_prob)
            # Give reward at the end of the episode
            if max_steps - steps == 1:
              rewards.append(reward)
            else:
              rewards.append(0)

          # for the environment
          # reset index, update paths
          env.i               += 2
          env.hr_states       = new_hr_states
          env.states          = new_states


        if reward >= 1:
          env.stats["Target found"] += 1

        list_log_probs = [log_prob.item() for log_prob in log_probs]
        if env.y == "+":
          if new_hr_state[0] == new_hr_state[env.i]:
            confidence_scores["true_closed"].append(sum(list_log_probs))
          elif new_hr_state[0] != new_hr_state[env.i]: #max_score([true_open]) < open_path.score --> open_path.verity = fake
            confidence_scores["true_open"].append(sum(list_log_probs)) 
        if env.y == "-":
          if new_hr_state[0] == new_hr_state[env.i]: # y = -, hr[0] == hr[-1] max_score([fake_closed]) < closed_path.score --> closed_path.verity = true
            confidence_scores["fake_closed"].append(sum(list_log_probs))
          elif new_hr_state[0] != new_hr_state[env.i]:
            confidence_scores["fake_open"].append(sum(list_log_probs))

        # stop training classifier with useless paths
        if sum(rewards) > 0:
          path = torch.tensor(new_state.flatten()).float()
          pred_y = env.classification_model(path)
          true_y = torch.tensor(np.array([float(env.y == "+")]))
          pred_y = pred_y.float()
          true_y = true_y.float()
          loss = env.loss_function(pred_y, true_y)
          loss = loss.float()
          env.classification_model.zero_grad()
          loss.backward()

        #reward, past_paths = diversity_reward(new_state[3:], past_paths)
        for i in range(4-len(rewards)):
          rewards.append(0)
                
        update_policy(policy_net, rewards, log_probs)
        all_rewards.append(np.sum(rewards)) # What to do with this?

        
        if (episode + 1) % 1000 == 0:
          mean_reward = np.mean(all_rewards[-1000:])
          print(episode, ":", mean_reward)
          avg_rewards.append(np.mean(all_rewards[-1000:]))
        
        if (episode + 1) % 10000 == 0:
          if not os.path.exists("models"):
            os.makedirs("models")
          net_name = str(episode+1) + "_ComplEx_policy_net"
          torch.save(policy_net, "models/" + net_name + ".pt")
          with open("log.txt", "a") as file:
            file.write(net_name + "\tMean reward for last 1K episodes: " + str(mean_reward) + "\n")
        
        
    print(env.stats)
    plt.plot(avg_rewards)
    plt.legend(['reward'])
    plt.xlabel('Episode')
    plt.title("avg rew, path len")
    plt.show()

    return policy_net, env.classification_model, confidence_scores







def clas_tag(env, clas_verdict):
  if env.y == "-":
    if clas_verdict:
      return "False positive"  
    else:
      return "True negative"
  elif env.y == "+":
    if clas_verdict:
      return "True positive"
    else:
      return "False negative"

def path_tag(path_success, path_complete):
  if path_success:
    if path_complete:
      return "True claim confirmed"
    else:
      return "False claim busted"
  else:
    if path_complete:
      return "False claim reproduced"
    else:
      return "Nothing accomplished"

def exclude_already_chosen_entities(new_ents, top_ent, scores):
  indices = [i for i, x in enumerate(new_ents) if x == top_ent]
  for i in indices:
    scores[i] = 0
  return scores

def exclude_already_chosen_paths(new_hr_paths, top_hr_path, scores):
  indices = [i for i, x in enumerate(new_hr_paths) if x == top_hr_path]
  for i in indices:
    scores[i] = 0
  return scores

def test_model(path_to_model, policy_net, classification_model, b, max_steps):

    policy_net.train = False
    number_of_successful_episodes = 0

    size_observation_space = 440 #(len(list(relevant_aliases.keys())))
    env = Environment_test("test.csv", path_to_model)
    size_action_space = len(env.action_space)
    
    max_episode_num = env.input.shape[0]
    env.stats["Number of episodes:"] = max_episode_num
    env.stats["Size of action space:"] = size_action_space
    max_steps = max_steps
    number_of_correct_classifications = 0

    path_results = {"True claim confirmed" : [],
                    "False claim busted" : [],
                    "False claim reproduced" : [],
                    "Nothing accomplished" : []}

    for episode in range(max_episode_num):
      
        hr_states, states = env.reset(i = episode)

        for steps in range(max_steps):
          new_hr_states     = []
          new_states        = []
          scores            = []
          actionsss         = []
          new_ents          = []
          top_hr_states     = []
          top_states        = []

          for hr_state, state in zip(env.hr_states, env.states):
            current_ent     = hr_state[env.i]
            actions, logps  = policy_net.get_actions(state, env.action_space, current_ent, env.kg, b)

            for action, logp in zip(actions, logps):
              new_hr_stater, new_stater, reward, new_enter, scorer = env.step(hr_state, state, action)

              # collect new states

              new_hr_states.extend(new_hr_stater)
              new_states.extend(new_stater)
              new_ents.extend(new_enter)

              # calculate heuristic
              scorer = np.asarray(scorer)
              scorer **= steps
              scorer += np.e**logp.item()
              #for i, sc in enumerate(scorer):
              #  scorer[i] = np.e** logp.item() + sc**steps
              
              scores.extend(scorer)
            
          scores = np.asarray(scores)
          ### Voting system ###
          scores_for_the_vote = []
          ### Voting system ###

          #kärpimine
          for i in range(b):
            top_score_index = scores.argmax()
            top_ent = new_ents[top_score_index]
            top_hr_state = new_hr_states[top_score_index]
            #top_hr_state[-1] = i
            top_hr_states.append(top_hr_state)
            top_states.append(new_states[top_score_index])

            ### Voting system #### using heuristics to vote
            scores_for_the_vote.append(scores[top_score_index]) 
            ### Voting system #### using heuristics to vote
            #scores[top_score_index] = 0
            
            # Previously, if an entity was chosen to extend one of the paths
            # that entity was excluded from being chosen for any other path
            # That was done to keep the agent from resulting in multiple
            # variations of the path "correct_relation and self_loops"
            # To facilitate the the voting mechanism, now only the specific
            # instance of a chosen entity is excluded so as not to pick the
            # exact same path again for another beam
            #scores[top_score_index] = 0
            scores = exclude_already_chosen_paths(new_hr_states, top_hr_state, scores)

          # for the environment
          # reset index, update paths
          env.i               += 2
          env.hr_states       = top_hr_states
          env.states          = top_states

        episode_success_flag = 0
        # statistics
        ### VOTING SYSTEM ###
        ents = []
        cnts = []
        ### VOTING SYSTEM ###


        for path, voting_score in zip(top_hr_states, scores_for_the_vote): # using heuristics to vote
          finish_ent = path[env.i]
          
          ### voting system ###
          if finish_ent in ents:
            iii = ents.index(finish_ent)
            cnts[iii] += 1
            #cnts[iii] += voting_score ### voting system, using heuristics to vote
          else:
            ents.append(finish_ent)
            cnts.append(1)
            #cnts.append(voting_score) ### voting system, using heuristics to vote
          ### voting system ###

          claim_targ = path[0]
          path_complete = finish_ent == claim_targ
          path_success = finish_ent in env.actual_target
          #if path_success:
          #  episode_success_flag = 1
          tag = path_tag(path_success, path_complete)
          path_results[tag].append((episode, path))
        
        ### VOTING SYSTEM ###
        ggg = cnts.index(max(cnts))
        guess = ents[ggg]
        episode_success_flag = int(guess in env.actual_target)
        ### VOTING SYSTEM ###

        number_of_successful_episodes += episode_success_flag

    print("Path results:")
    for key in path_results:
      print(key + ":", len(path_results[key]))

    accuracy = number_of_successful_episodes / max_episode_num
    return path_results, accuracy

#os.chdir("/content/drive/MyDrive/Colab Notebooks/thesis/NELL-995")
for i, o in enumerate(os.listdir("./models")):
  print(i, o)

policy_net_path = os.listdir("./models")[19]
policy_net = torch.load("./models/"+policy_net_path)
start = int(policy_net_path.split("_")[0])
start

path_results_ = []
max_steps = 3
start = start
max_episode_num = 5*10**
b = 1
rs = 1
ms = 5
policy_net, classification_model, confidence_scores = train_model("ComplEx", max_episode_num, max_steps, random_state = rs, manual_seed = ms, policy_net = policy_net, start = start)
bees = [3, 5, 10]
for b in bees:
  path_results, accuracy = test_model("ComplEx", policy_net, classification_model, b, max_steps)
  print("Beam number:", b)
  print("Vote accrcy:", accuracy)
torch.save(policy_net, "300k_top3Action_bestEntity.pt")

# algoritmi tuleb timmida niimoodi, et kui hinnatakse pathi unikaalsust, siis selfLoope arvesse ei võeta
# nt [Temake, selfLoop, Temake, ütles, Seda] == [Temake, ütles, Seda, selfLoop]
for e in path_results["Nothing accomplished"]:
  print(e)

#RUN ME
path_results_ = []
accuracy_ = []
max_steps = 3
rs = 1
ms = 5
#policy_net = torch.load("100k_top3Action_bestEntity.pt")
for b in [3]:#, 5, 10]:
  print(b)
  path_results, accuracy = test_model("ComplEx", policy_net, None, b, max_steps)
  path_results_.append(path_results)
  accuracy_.append(accuracy)
  print(path_results)
  print(accuracy)

pickle.dump(path_results_, open("path_results.pkl", "wb"))
pickle.dump(accuracy_, open("accuracy.pkl", "wb"))

for accuracy, path_results in zip(accuracy_, path_results_):
  keys = list(path_results.keys())
  amounts = [len(path_results[key]) for key in keys]
  overall_acc = sum(amounts[:2]) / sum(amounts[2:])
  print("overall_acc:", round(overall_acc,3))
  print("acc:", round(accuracy,3))
  print()

print(amounts[:2])

b = 3
rs = 1
ms = 5
policy_net, classification_model, confidence_scores = train_model("ComplEx", max_episode_num, max_steps, random_state = rs, manual_seed = ms)#,  policy_net = policy_net, start = start)
path_results = test_model("ComplEx", policy_net, classification_model, b, max_steps)

torch.save(policy_net, "100k_top3Action_bestEntity.pt")

path_results, accuracy = path_results
# accuracy on votingu tulemuste täpsus ehk siis mood(path_search(masin_õpe)) = kõik_juhtumid
# path results on kõikide otsingutulemuste kogum
# Ehk siis path results on üleüldine täpsus intelligentse heuristikaga otsingualgoritmile
# Hääletustulemuste täpsust ehk siis demokraatliku otsuse efektiivsusmäär
# Demokraatliku otsuse täpsus - Democratic verdict accuracy

path_results = test_model("ComplEx", policy_net, classification_model, b, max_steps)

path_results

!ls

bees = [10]
max_steps = 3

path_resultses = []
#policy_net = torch.load("100k_top3Action_bestEntity.pt")
path = "ComplEx"
for b in bees:
  print("100K ComplEx, top", b, "actions and entities")
  path_results, accuracy = test_model(path, policy_net, None, b, max_steps)
  print("accuracy:", accuracy)
  path_resultses.append(path_results)

bees = [10]
max_steps = 3

path_resultses = []
#policy_net = torch.load("100k_top3Action_bestEntity.pt")
print("Vote = 1 for everyone./nLast one was path final ent complex_score + prob for final action")
path = "ComplEx"
for b in bees:
  print("100K ComplEx, top", b, "actions and entities")
  path_results, accuracy = test_model(path, policy_net, None, b, max_steps)
  print("accuracy:", accuracy)
  path_resultses.append(path_results)

path_results, accuracy = test_model(path, policy_net, None, b, max_steps)
print("accuracy:", accuracy)
path_resultses.append(path_results)

b = 10
path_results = path_resultses[2]
print("b:", b)
for key in path_results:
  print(key)
  for row in path_results[key]:
    print(row)
print("______________________________________________________________________________________________________________________________")

path_results_3 = path_resultses[0]
key = "False claim busted"
results = path_results_3[key]
for result in results:
  print(result)

bees = [3, 5, 10]
for b, path_results in zip(bees, path_resultses):
  print(b)
  for key in path_results:
    print(key)
    counts = {}
    for p in path_results[key]:
      id, path = p
      confidence_rank = path[-1]
      if confidence_rank in counts:
        counts[confidence_rank] += 1
      else: counts[confidence_rank] = 1
    keys = list(counts.keys())
    keys.sort()
    total = 0
    for key in keys:
      total += counts[key]
    for key in keys:
      proportion = round(counts[key] / total, 2)
      print(key, proportion)
    print()

import numpy as np
from numpy import dot
from numpy.linalg import norm
def split_embedding(embedding):
  real = embedding[:20]
  imag = embedding[20:]
  embedding = (real, imag)
  return embedding

def cosine_similarity(one, two):    
  return dot(one, two) / (norm(one)*norm(two))

def embeddings_for_index(i):
  triple = kg.iloc[i]
  source, relation, target = triple
  source_e, target_e = model.get_embeddings([source, target])
  relation_e = model.get_embeddings(relation, "relation")
  return source_e, relation_e, target_e

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

def complexify(input_vector):
  output_vector = []
  for real, imag in zip(input_vector[:20], input_vector[20:]):
    comp = complex(real, imag)
    output_vector.append(comp)
  output_vector = np.asarray(output_vector)
  return output_vector

def dot_product(source, relation, target):
  target = target.conjugate()
  dot_product = 0
  for u, a, e in zip(source, relation, target):
    dot_product += u * a * e
  return dot_product.real

def embeddings_for_index(i):
  triple = kg.iloc[i]
  source, relation, target = triple
  source_e, target_e = model.get_embeddings([source, target])
  relation_e = model.get_embeddings(relation, "relation")
  return source_e, target_e, relation_e

asp =pickle.load(open("action_space.pkl", "rb"))
asp.remove("major_field_of_study_inv")
#asp.append("languages_inv")
pickle.dump(asp, open("action_space.pkl", "wb"))

resultant_path_keys = ["True claim confirmed", "False claim busted"]
ids = []
for rpk in resultant_path_keys:
  for p in path_results[rpk]:
    id, _ = p
    ids.append(id)
ids.sort()
failed_indices = np.setdiff1d([*range(168)], ids)
failed_indices

resultant_path_keys = ["True claim confirmed", "False claim busted"]
relation_indices = [3, 5, 7]
the_models_action_choices = []

for path_results in path_resultses:
  action_choices = []
  for rp_key in resultant_path_keys:
    a_list_of_tuples_id_and_path = path_results[rp_key]
    for tup in a_list_of_tuples_id_and_path:
      id, path = tup
      relations_of_a_path = []
      for ri in relation_indices:
        a_relation = path[ri]
        if a_relation != "selfLoop":
          relations_of_a_path.append(a_relation)
      action_choices.append(relations_of_a_path)
  the_models_action_choices.append(action_choices)
"""
for path_results in path_resultses:
  for result in path_results:
    print(result)
    for thing in path_results[result]:
      print(thing)
    
  print()
"""

for acs in the_models_action_choices:
  acs_singles = []
  count = []
  for roap in acs:
    if roap in acs_singles:
      count[acs_singles.index(roap)] += 1
    else:
      acs_singles.append(roap)
      count.append(1)
  for rOAP, cOUNT in zip(acs_singles, count):
    print(rOAP, cOUNT)
  print()

max_steps = 3
iterations = 300
b = 3
rs = 1
ms = 5
paths = ["ComplEx"] # ["TransE"], ["TransE", "ComplEx"], ["ComplEx"]
for path in paths:
  print(path, "b:", b)
  policy_net, classification_model, confidence_scores = train_model(path, iterations, max_steps, random_state = rs, manual_seed = ms, policy_net = policy_net)
  #path_results = test_model(path, policy_net, classification_model, b, max_steps)
  #print()
  torch.save(policy_net, "300k_top3Action_bestEntity.pt")

bees = [3, 5, 10]

path_resultses = []
print("300K ComplEx, top 3 actions, best entity")
for b in bees:
  path_results, accuracy = test_model(path, policy_net, classification_model, b, max_steps)
  print(b, ":", accuracy)
  path_resultses.append(path_results)

pickle.dump(path_resultses, open("path_resultses_300k_3_5_10.pkl", "wb"))

bees = [3, 5, 10]

path_resultses = []
print("300K ComplEx, top 3 actions, best entity")
for b in bees:
  path_results, accuracy = test_model(path, policy_net, classification_model, b, max_steps)
  print(b, ":", accuracy)
  path_resultses.append(path_results)

pickle.dump(path_resultses, open("path_resultses_300k_3_5_10.pkl", "wb"))

for path_results in path_resultses:
  for pr in path_results:
    print(pr)
    for p in path_results[pr]:
      print(p)
  print()



## READING KNOWLEDGE GRAPH ###

def train_pos():

  t = {}

  with open("train_pos") as file:
    for l in file:
      l = l.strip().split("\t")
      source = l[0]
      target = l[1]
      if source in t:
        t[source].append(target)
      else: t[source] = [target]
  
  return t

def read_triples():


  t = {"e0":[], "r":[], "e1":[]}

  with open("graph.txt") as file:
    for l in file:
      l = l.strip().split("\t")
      t["e0"].append(l[0])
      t["r"].append(l[1].split("/")[-1]) # SHORTEN THE RELATION WITH .split("/")[-1]
      t["e1"].append(l[2])

  with open("train_pos") as file:
    for l in file:
      l = l.strip().split("\t")
      t["e0"].append(l[0])
      t["e1"].append(l[1])
      t["r"].append(l[2].split("/")[-1]) # SHORTEN THE RELATION WITH .split("/")[-1]

  return pd.DataFrame(t)

def read_kg_triples():
  t = {}

  with open("graph.txt") as file:
    all_entities = set()
    for l in file:
      l = l.strip().split("\t")
      
      e0 = l[0]
      r = l[1].split("/")[-1]
      e1 = l[2]

      if e0 == e1:
        continue

      #Collect all entities, use to generate self loops
      all_entities.add(e0)
      all_entities.add(e1)

      e0key = e0+r

      if e0key in t:
        t[e0key].append(e1)
      else:
        t[e0key] = [e1]
      
    all_entities = list(all_entities)
    for e in all_entities:
      t[e+"selfLoop"] = [e]

  return t

#¤¤ READING SOURCE AND TARGET PAIRS FOR TRAINING ¤¤#
def read_input():
  p = {"es":[], "rr": [], "et":[], "y":[]}
  root_relation = os.getcwd().split("@")[-1]  # Kas kaasata ka root_relation + "_inv" ehk tagurpidine suhe
  root_relation

  with open("train.pairs") as file:
    for l in file:
      l = l.replace("thing$", "/")
      l = l.replace(": ", ",")
      l = l.strip().split(",")
      p["es"].append(l[0].replace("_", "/", 1))
      p["rr"].append(root_relation)
      p["et"].append(l[1].replace("_", "/", 1))
      p["y"].append(l[2])

  return pd.DataFrame(p)